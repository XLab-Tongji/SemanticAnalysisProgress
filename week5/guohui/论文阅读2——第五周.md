##An Attention Pooling based Representation Learning Method for Speech Emotion Recognition

###Abstract

* 基于 attention pooling 的语音情感识别（SER）表示学习方法。

  * 通过将深度卷积神经网络（CNN）直接应用于从语音中提取的频谱图，以端到端的方式学习情绪表示。
  * 在GoogleNet推动下，两组具有不同形状的滤波器从输入频谱图捕获时域和频域上下文信息。
  * 学习到的特征被连接起来并送入后续的卷积层。
  * 为了学习最终的情感表征，进一步提出了一种新的注意力集中方法。
    * 与现有的池化方法（例如最大池化和平均池化）相比，所提出的注意力池可以有效地将类别不可知的自下而上和类别特定的自上而下的注意力映射结合起来。

* 我们对基准IEMOCAP数据进行了广泛的评估，用以评估提出的representation的有效性。

* 结果：在四种情绪中，加权准确度（WA）：71.8%

  未加权准确度（UA）：68.8％，

  超过现有最先进的技术：WA：3％，UA：4％。



###Introduction

* SER系统成功的关键：找到语音段的有效情感表示（情绪表达的复杂性和缺乏大型数据集）

* 传统的SER方法：

  * 前端：基于帧的特征提取
  * 后端：用于分类或回归的语音表达
  * [1]Slaney：将高斯混合模型（GMM）应用于SER 的Mel频率倒谱系数（MFCC）。
  * [2] [3]：提取韵律特征以训练支持向量机（SVM）分类器。

  这些基于手工制作的特征：表征语音中的情感信息——不满意

* [6, 7, 8, 9, 10, 11, 12]：几种基于深度神经网络（DNN）或卷积神经网络（CNN）的SER方法

  * [6,7]：应用了多阶段程序，其中DNN和CNN网络被训练用于前端特征提取，接着是后端情绪识别器，例如SVM和极端学习机（ELM）。
  * [9,11]：端到端的训练模式：
    * Trigeorgis [9]：将原始音频送到CNN用于前端特征提取，然后是用于情绪表示学习的长短期记忆（LSTM）层。(可以使用反向传播算法联合优化模型参数。)
  * [8,10]：应用max-pooling操作从显著区域获取语音表示。
  * Neumann [12]：在最大池操作之后进一步引入了attention机制。 
  * Mirsamadi[13]：将加权pooling应用于RNN输出。

* 近来基于深度学习的SER方法的问题：

  * 语音情感信息可以在时域和频域中体现——不清楚如何设计合适的神经网络架构以利用时间和频率信息来导出有效的语音情感表示
    * [14,15]：2D时频（TF）LSTM和Grid-LSTM来对大规模自动语音识别（ASR）随时间和频率的变化进行建模。
    * 复杂的模型结构容易在小规模数据集上过度拟合（例如IEMOCAP [16]）
  * 简单地average-pooling或max-pooling可能不足以得出需要分析更高阶统计量的复杂情绪表达的有效表示。
    * [12,13,10]：引入表示学习的attention机制（但是它们通常以自下而上的方式从特征中获得显著区域）

* 提出一种基于attention pooling的SER表示学习方法：

  ![](https://github.com/guohui15661353950/pictures/blob/master/屏幕快照%202019-04-01%20下午10.18.20.png?raw=true)

  * 将深度CNN直接应用于从语音提取的频谱图，具有不同形状的两组卷积滤波器以捕获时域和频域上下文信息（卷积滤波器形状可能会影响情绪表示的有效性）
  * 在GoogleNet的推动下，学习特征被进一步连接起来并送入以下卷积层。
  * 提出一种新的attention pooling的方法：
    * 两个attention map（class-agnostic和class-specific）：
      * 第一个attention map：以自下而上的方式导出
      * 第二个attention map：与情绪类型直接相关
  * 数据集：IEMOCAP
  * 结果：WA的SER性能为71.8％，UA的SER性能为68.0％，两者均显著优于现有的最先进方法。



###Attention pooling based representation learning

特征提取过程、SER的CNN架构、两组卷积滤波器（用于捕获特定时间和来自输入光谱图的特定频率的上下文信息）、attention pooling的推导。

####2.1 Input spectrogram calculation

* CNN的输入是从语音中提取的频谱图：

  * 给定一段语音，我们将其分为2个段进行训练，并使用1s的重叠使我们能够获得更多的训练数据。
  * 每个段有与相应语音相同的标签。
  * 对于测试语音，重叠设置为1.6秒，并且通过对每个片段的分数求平均来获得最终预测。

* 为每个语音段计算频谱图：

  * 将一系列重叠的40ms汉明窗口应用于语音信号，移位为10ms。

  * DFT：长度为1600（对应于10Hz网格分辨率）

  * 输入特征：DFT为0到4kHz的频率范围

  * 频谱图：400*200矩阵

  * 标准化：将其线性转换为[-1,1]的范围，将μ-law扩展应用于每个x：$F(x)=sgn(x)\cdot ln(1+\mu \left | x \right |)/ln(1+\mu ) (-1\leqslant x\leqslant 1)$

    其中μ= 255。与对数频谱图后处理相比，μ-law扩展将缩小最大值和最小值之间的差异，这实际上提高了训练过程的稳定性。

####2.2 CNN architecture

* 输入feature map：向量$（H，W，C_{in}）$，其中H是feature map的高度，W是宽度，$C_{in}$是通道数。

* 标准CNN结构将通过多个卷积层处理它，其中卷积层计算第k个输出feature map$C_{out_k}$，如下所示

  $C_{out_k}=b_i+\sum_{i=1}^{C_{in}}\omega (C_{out_k},i)*input(C_{in_i})$

  * b:偏差，ω:权重矩阵，*:卷积运算。 
  * CNN滤波器对输入的feature map进行卷积，并输出其学习表示。 在CNN结构中，可以通过应用多个卷积层来学习高级表示。

* 一旦获得了从不同时频域学习的特征，CNN的下一步就是提取用于情感识别的高级表示。
  使用4个卷积层和3×3大小的滤波器，并使用2×2的max-pooling对feature maps进行下采样。

####2.3. Time-specific and frequency-specific filters designed for input spectrogram

* 第一个卷积层的作用是从原始频谱图中提取特征。
  * 由于卷积滤波器在每个位置接收矩形部分，这意味着每个输出包含特定范围的时频信息。
  * 该矩形 时间×频率窗口也称为接受区。
* Conv1滤波器形状设计的过程：
  * 将频率轴上的卷积滤波器的接受区设置为最小值2，然后调整时间轴上的接受区（找到主要限制情绪表示的时间跨度，同时最小化频率信息的影响）
  * 将类似的过程应用于频率轴。
  * channel concatenation应用于时间特定和频率特定的feature map（利用时间和频率信息）

####2.4. Attention pooling method

* 通常情况：CNN使用几个完全连接（FC）层来在目标标签上产生概率分数。

  * 直接连接卷积特征并将其送到FC层可能导致过度参数化，这使得训练变得困难，尤其是对于小规模数据集。

* 可以对feature map进行下采样并保持高级特征的表示能力的pooling：

  * 两种不同的pooling方法：

    * Global average pooling

      * 覆盖整个feature map的pooling窗口

      * 对于向量特征为$(HW)\times C$的feature map X510/5000（其中HW表示重构的的feature map，C表示通道大小），第k个预测分数为：$score(k)=1^{T}XW_k$

        其中$W_k$是$C×1$类特定的完全连接的权重矩阵，1是所有值都是1的向量，$1^T X$是输入特征的总和。

      CNN中常用的方法是全局平均合并（GAP），其对sum-pooled特征取平均值并输出1×1×C特征向量。

    * Attention pooling

      * Global average pooling的局限：平等对待所有输入数据，不考虑区域显著性。在频谱图中，许多区域可能与情绪信息关系不大。
      * Attention pooling：在Conv5之后引入attention pooling层
        * Attention pooling基于second-order pooling，对细粒度分类任务有用。
        * 首先构造具有C×C的特征$X^T X$来实现second-order pooling；然后使用C×C完全连接的权重矩阵，并用trace替换内积。
          第k个类预测：$score(k)=Tr(X^TXW_k^T)$
        * $W_k$是高维的，当数据集大小不足时导致过度拟合问题——需要低秩估计
          * 权重矩阵$W_k$被分解为两个C×1向量的乘积：$score(k)=Tr(X^TXb_ka_k^T)$
          * （根据 trace操作的循环属性：$Tr(ABC) = Tr(CAB)​$，标量的trace是其本身）$$score(k)=Tr(X^TXW_k^T)
            =a_k^TX^TX_{b_k}
            =(X_{a_k})^T(X_{b_k})​$$
          * 第k个类的预测分数可以被视为两个attention map $X_{a_k}$和$X_{b_k}$的组合。可以将$X_{b_{k}}$设置为类不可知，而不是使它们都是类特定的，因此$bk = b$。然后通过结合类特定的自上而下的attention $X_{a_k}$和类不可知的自下而上的attention $X_{b}$，可以获得最终的注意力模型：$$score(k)=(X_{a_k})^T(X_{b})$$
          * 也可以通过element-wise乘法和global pooling实现：$score(k)=1^{T}(X_{a_k}\circ X_b)$



###Experiments and Analysis

####3.1. Experiment setting

* 数据集：IEMOCAP
  * 为5个部分，每个部分包含一个男性和一个女性演员。
  * 根据记录场景，数据可以进一步分为improvised语音部分和scripted语音部分。
  * 数据集中的每段语音由多个注释器注释为8个情感标签。
  * 从improvised语音的研究中选择4种情绪类型（即中性，快乐，愤怒和悲伤），因为scripted语音数据可能包含不需要的上下文信息。
* 采用leave-one-out策略进行10倍交叉验证
  * 在每个训练过程中，9个speaker用作训练数据，剩下的一个用于测试数据。
  * CNN训练：PyTorch深度学习框架
  * 优化方法：标准随机梯度下降（mini-batch size：64）
  * Nesterov动量：0.9    权重减少：0.0001
  * CNN训练超过50个epochs
  * 初始学习率：0.05     （在21,31和41个epoch减少了10倍）
  * 我们在每个卷积层之后采用批量标准化层，并且使用的激活函数是整流线性单元（ReLU）。
  * 用于优化的目标函数：使用交叉熵（CE）标准。
* 评估SER性能的指标：
  * 加权准确度（WA），是所有语音的分类准确度。
  * 未加权准确度（UA），平均每个情绪类别的准确性。

####3.2. Experimental with time-specific and frequency-specific filters

* 为了在排除其他因素的同时找到最佳滤波器形状，第一卷积层是独立测试的。

* 我们直接将GAP应用于feature map，并使用完全连接的图层（16：4）来生成预测分数。

* ![](https://github.com/guohui15661353950/pictures/blob/master/屏幕快照%202019-04-02%20上午9.11.36.png?raw=true)

  * 当增加高度（频率）时，WA和neutral情感类的精度也会增加。然而，该趋势在大约10（100Hz）的高度处变平，超过此高度，接收区频率范围的进一步增加没有显着改善。
  * 当增加宽度（时间）时，WA首先增加到大约8（80ms）宽度的峰值，然后逐渐减小。有趣的是，当接收区时间跨度变大时，angry类的准确性迅速下降，这表明angry的情绪是一种短时表达。
  * 快乐类精度最差并且与滤波器形状无关，因此为了清楚起见从图中排除。 这可能表明happy类由更高级别的representation学习。

* 基于WA结果，我们分别选择10×2和2×8的最佳滤波器形状，并用Conv1中的独立通道将其连接。

  通道连接有两个方面：

  ​	后面的层能够在时域和临时域中接收大区域，为更高级别的表示学习提供更多信息。

  ​	与在两个维度上使用大型过滤器相比，这显著地减少了参数并因此减少了过度拟合的机会

####3.3 Experimental with different pooling methods

* 评估attention pooling方法：三种不同的pooling方法：

  * CNN TF GAP：
    * baseline CNN结构。
    * 使用8通道2×8滤波器和8通道10×2滤波器。然后通过Conv1中的通道连接它们。
  * CNN TF Bilinear：
    * 使用second-order pooling替换GAP
    * 由Conv5生成的feature map乘以其转置得到6400维特征，然后是l2标准化和带符号的平方根。最后，使用完全连接的层（6400：4）来得到最终预测分数。
  * CNN TF Att.pooling：
    * 用attention pooling替换GAP
    * 我们在Conv5之后使用1×1卷积层生成自上而下的feature map（大小为H×W×4，对应于4个情感标签），并使用另一个1×1卷积层生成自下而上的feature map（大小为H×W×1）。
    * 然后将softmax操作应用于自下而上的feature map。最后，两种类型的feature map逐元素相乘并在空间上平均，以获得所有4种情绪类别的预测分数。

  ![](https://github.com/guohui15661353950/pictures/blob/master/屏幕快照%202019-04-02%20上午9.27.35.png?raw=true)

* CNN TF Bilinear的表现略差于CNN TF GAP baseline（由于完全连接层中的大量参数引起的过度拟合）

  attentional pooling的表现：CNN TF Att.pooling——最高的WA和UA

  过滤器由他们的WA分数选择。

  更仔细的选择（例如考虑特定情绪的表现）可以进一步提高准确性。



###Conclusion

* 针对SER任务提出了基于attention pooling的CNN。
  * 实验证明滤波器形状在情绪表达学习中起着重要作用（因此为第一层定义不同的形状滤波器以捕获时间和频率表示）
  * 进一步引入attention map，以模拟时间和频率显著性。
* 所提出的CNN具有更优的WA和UA，分别达到71.75％和68.06％，比之前的分别增加了约3％和4％。
  这些结果证明了所提出的CNN结构具有强烈的情感表征能力。
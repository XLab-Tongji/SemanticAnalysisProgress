## 2019-04-16～2019-04-23 week8进度总结
###  苏昭帆
1. 将实时检测语音情感的雷达图整合到主代码中

2. 检查了训练过的svm模型，本地这边没有出现分类概率始终不变的情况

3. 用邹笑寒提供的音频进行了测试，效果不是很好。自己听了一下这些音频，音频本身的情感表现不是很明显，有些感觉比较混淆，希望大家能一起评估一下

4. 使用郭辉提供的自己录制的电视剧音频进行了测试，能够实现对长音频的实时情感分析，长音频主情感分析较为准确，实时情感会有变动

5. 重新整理查看了当时整合模型时的代码，发现了可能是导致分类概率始终不变的问题所在，代码更改之后已经提交

6. 正在看关于音频MFCC特征的介绍、提取方法，感觉可以从这方面入手，因为svm模型训练的方法基本固定，只能改变喂进去的数据了


### 刘娅璇/刘岳涵  
1. 代码：将对话检索模型封装成chatting分类器加入项目，已提交至分支"liuyuehan&liuyaxuan"；在原对话模型上添加了基于词频和tfidf特征值向量的训练方法，提高了对话检索能力，详见**[ConversationDemo](https://github.com/Masquerade51256/ConversationDemo.git)**仓库readme文件。
   
2.现存问题：模型训练不稳定，根据训练样本顺序不同导致训练结果差异较大；代码结构较乱。


### 邹笑寒
1. 数据集收集：试图找到 [CHEAVD](https://link.springer.com/article/10.1007/s12652-016-0406-z) 数据集，给数据集的建立者和一些其他论文作者发了邮件，目前还没收到回复；CHEAVD 来源于中文影视剧中所截取的音视频片段，带情感标注，发布于 [MEC 2016](https://link.springer.com/chapter/10.1007/978-981-10-3005-5_55) 比赛期间（[MEC 2017](http://www.chineseldc.org/htdocsEn/emotion.html) 期间发布了进化版数据集 CHEAVD 2.0，也没找到）。

2. 论文阅读：阅读了一些论文和代码，准备改进特征提取方式。


### 郭辉
1. 收集数据：在电影或电视剧中收集录制各种情感的音频，但是工作量较大，目前只有短音频100多个和长音频十多个音频文件，而且每种情感数据量差异较大，所以还不能作为数据集用来训练。长音频用来测试是否可以试试绘制雷达图，效果还可以。短音频集将不断扩充。

   地址：长音频：<https://github.com/guohui15661353950/audio-data/tree/master/长音频>

   ​	    短音频：<https://github.com/guohui15661353950/audio-data/tree/master/短音频>(还未全部上传)

2. 阅读论文：《An Ensemble Framework of Voice-Based Emotion Recognition System for Films and TV Programs》论文地址：<https://arxiv.org/abs/1803.01122>

### 杨慧宇/刘雨
1. 实验：使用“微博embeddings(Word+Character+Ngram)”重新训练分类器A、C、D，在单独测试的情况下，最佳准确率分别在94%、100%、100%。但整合到实际使用的语境中，进行人工对话(Command模式)时，测试的正确率不高。






